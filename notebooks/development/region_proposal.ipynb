{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing the Region Proposal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro, Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Region proposal network was introduced in faster RCNN. Its role is to predict bounding boxes for objects in images. It works by sliding over the final feature map generated by a base convolutional network. At every point, it computes \"objectness\" scores and regression scores for a selection of anchor boxes. Here, we work on developing a region proposal network (RPN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import ops \n",
    "import einops\n",
    "from itertools import product\n",
    "from typing import List\n",
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn import functional as F "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to our network is a tensor of shape (b, 3, 1024, 1024). The final feature map has dimensions (b, 1024, 32, 32). Let us generate a dummy feature map tensor: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "input_size = 1024\n",
    "feature_map_size = 32 \n",
    "feature_dim = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 32, 32])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_map = torch.randn((batch_size, feature_dim, *(feature_map_size,) * 2))\n",
    "feature_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anchor Boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first job is to associate every point of the feature map with a number of \"anchor boxes\" upon which the RPN performs classification and bounding box regression. The anchor boxes are a fixed attribute of the RPN. Since the input image is downsampled by a scale of `d = input_size / feature_map_size`, the corresponding base anchor box for a point (i, j) on the feature map is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = input_size / feature_map_size\n",
    "\n",
    "def base_image_(i, j):\n",
    "    \n",
    "    x = i * d\n",
    "    y = j * d\n",
    "    w = d \n",
    "    h = d \n",
    "\n",
    "    return x, y, h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we can create the tensor of base images: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = product(range(feature_map_size), range(feature_map_size))\n",
    "\n",
    "base_boxes = torch.zeros((32, 32, 4))\n",
    "\n",
    "for i, j in indices:\n",
    "    base_boxes[i, j, :] = torch.tensor(base_image_(i, j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY_CHECKS\n",
    "\n",
    "test_input = torch.zeros((1024, 1024), dtype=bool)\n",
    "\n",
    "indices = product(range(feature_map_size), range(feature_map_size))\n",
    "for i, j in indices:\n",
    "    \n",
    "    x, y, w, h = base_boxes[i, j, :]\n",
    "    x = x.long().item()\n",
    "    y = y.long().item()\n",
    "    w = w.long().item()\n",
    "    h = h.long().item()\n",
    "    \n",
    "    test_input[ x : x + w, y : y + w ] = True\n",
    "    \n",
    "torch.all(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base the base image is a tensor of shape (feature_map_shape, feature_map_shape, 4). However, these are just the base anchor boxes. From these anchor boxes, we can create more anchor boxes by adjusting the scale and aspect ratio of these boxes. In the original paper, 3 scales and aspect ratios are specified. Let us give some sample scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales = [2, 4, 8]\n",
    "aspect_ratios = [1, 2, 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate the anchor boxes, we will work from the base anchor box. We first change the box format to specifying the center point, which will make the calculations much easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16., 16., 32., 32.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten\n",
    "base_boxes_converted = einops.rearrange(base_boxes, 'h w l -> ( h w ) l')\n",
    "# convert \n",
    "base_boxes_converted = ops.box_convert(base_boxes_converted, 'xywh', 'cxcywh')\n",
    "# unflatten\n",
    "base_boxes_converted = einops.rearrange(base_boxes_converted, '( h w ) l -> h w l', h = feature_map_size, w = feature_map_size)\n",
    "base_boxes_converted[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the first two box coordinates specify the center of the box. We need only change the height and width!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the new height and width can be difficult. Assuming that the aspect ratio is 1, the scale simply multiplies the width and height of the base image. However, when the aspect ratio is not 1, the width and height must be calculated so that ` width/height = aspect ratio ` while the areas are the same as the square that you would get if the aspect ratio is 1, that is ` width * height = (base_width * scale) ^ 2 `. From this, we can derive the equations used in the implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anchor_box_from_base(base_boxes, scale, aspect_ratio):\n",
    "    \n",
    "    x, y, w, h = base_boxes[0, 0, :]\n",
    "\n",
    "    w_new = int( ( aspect_ratio **.5 ) * scale * w )\n",
    "    h_new =  int( scale * w / ( aspect_ratio ** .5 ) )\n",
    "\n",
    "    anchor_boxes = torch.zeros_like(base_boxes)\n",
    "    \n",
    "    indices = product(\n",
    "        range(anchor_boxes.shape[0]), \n",
    "        range(anchor_boxes.shape[1])\n",
    "    )\n",
    "    \n",
    "    for i, j in indices: \n",
    "        \n",
    "        x, y, _, _ = base_boxes[i, j, :]\n",
    "        anchor_boxes[i, j, :] = torch.tensor([x, y, w_new, h_new])\n",
    "\n",
    "    return anchor_boxes\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's perform some quick sanity checks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SANITY CHECKS\n",
    "assert torch.all( base_boxes_converted == get_anchor_box_from_base( base_boxes_converted, 1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 16.,  16., 181.,  90.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors = get_anchor_box_from_base( base_boxes_converted, 4, 2)\n",
    "anchors[0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aspect ratio looks good, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16290 16384\n"
     ]
    }
   ],
   "source": [
    "print( 181 * 90, 32 * 4 * 32 * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale looks good too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply the function to each combination of scale and aspect ratio to create the desired anchor boxes. We will add an extra dimension and then concatenate them together so we end up with an anchor box tensor of shape ( num_features, num_features, k, 4 ), where k is the number of scale/aspect ratio combos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_boxes = []\n",
    "\n",
    "for scale, aspect_ratio in product(scales, aspect_ratios):\n",
    "    \n",
    "    anchors = get_anchor_box_from_base(base_boxes_converted, scale, aspect_ratio)\n",
    "    \n",
    "    anchors = einops.repeat(\n",
    "        anchors, \n",
    "        'n_features_1 n_features_2 four -> n_features_1 n_features_2 1 four', \n",
    "        n_features_1 = feature_map_size, \n",
    "        n_features_2 = feature_map_size, \n",
    "        four = 4, \n",
    "    )\n",
    "    \n",
    "    anchor_boxes.append(anchors)\n",
    "    \n",
    "anchor_boxes = torch.concat(anchor_boxes, dim = 2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert all the boxes back to the desired format, which is 'xywh'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, n2, k, four = anchor_boxes.shape\n",
    "\n",
    "anchor_boxes = einops.rearrange(\n",
    "    anchor_boxes, \n",
    "    'n1 n2 k four -> ( n1 n2 k ) four'\n",
    ")\n",
    "\n",
    "anchor_boxes = ops.box_convert( anchor_boxes, 'cxcywh', 'xywh')\n",
    "\n",
    "anchor_boxes = einops.rearrange(\n",
    "    anchor_boxes,\n",
    "    '( n1 n2 k ) four -> n1 n2 k four', \n",
    "    n1=n1, n2=n2, k=k, four=four\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16., -16.,  64.,  64.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_boxes[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, what we end up with are some boxes which are out of the image bounds. This won't do - let's clip them. This requires conversion to 'xyxy' format again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_boxes = einops.rearrange(\n",
    "    anchor_boxes, \n",
    "    'n1 n2 k four -> ( n1 n2 k ) four'\n",
    ")\n",
    "\n",
    "anchor_boxes = ops.box_convert( anchor_boxes, 'xywh', 'xyxy')\n",
    "\n",
    "anchor_boxes = einops.rearrange(\n",
    "    anchor_boxes,\n",
    "    '( n1 n2 k ) four -> n1 n2 k four', \n",
    "    n1=n1, n2=n2, k=k, four=four\n",
    ")\n",
    "\n",
    "anchor_boxes = ops.clip_boxes_to_image(anchor_boxes, (input_size, input_size))\n",
    "\n",
    "anchor_boxes = einops.rearrange(\n",
    "    anchor_boxes, \n",
    "    'n1 n2 k four -> ( n1 n2 k ) four'\n",
    ")\n",
    "\n",
    "anchor_boxes = ops.box_convert( anchor_boxes, 'xyxy', 'xywh')\n",
    "\n",
    "anchor_boxes = einops.rearrange(\n",
    "    anchor_boxes,\n",
    "    '( n1 n2 k ) four -> n1 n2 k four', \n",
    "    n1=n1, n2=n2, k=k, four=four\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  0., 48., 48.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_boxes[0, 0, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the creation of our anchor box tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anchor Box Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anchor_boxes(input_size: int, feature_map_size: int,\n",
    "                        scales: List[float], aspect_ratios: List[float]):\n",
    "    \n",
    "    d = input_size / feature_map_size\n",
    "\n",
    "    # CREATE THE BASE IMAGE\n",
    "    def base_image_(i, j):\n",
    "        \n",
    "        x = i * d\n",
    "        y = j * d\n",
    "        w = d \n",
    "        h = d \n",
    "\n",
    "        return x, y, h, w\n",
    "    \n",
    "    indices = product(range(feature_map_size), range(feature_map_size))\n",
    "\n",
    "    base_boxes = torch.zeros((32, 32, 4))\n",
    "\n",
    "    for i, j in indices:\n",
    "        base_boxes[i, j, :] = torch.tensor(base_image_(i, j))\n",
    "        \n",
    "    # CONVERT TO CXCYWH\n",
    "    base_boxes_converted = einops.rearrange(\n",
    "        base_boxes, 'h w l -> ( h w ) l'\n",
    "    )\n",
    "    base_boxes_converted = ops.box_convert(\n",
    "        base_boxes_converted, 'xywh', 'cxcywh'\n",
    "    )\n",
    "    base_boxes_converted = einops.rearrange(\n",
    "        base_boxes_converted, '( h w ) l -> h w l', \n",
    "        h = feature_map_size, w = feature_map_size\n",
    "    )\n",
    "\n",
    "    # CREATE THE ANCHOR BOXES FROM THE BASE\n",
    "    def get_anchor_box_from_base(base_boxes, scale, aspect_ratio):\n",
    "    \n",
    "        x, y, w, h = base_boxes[0, 0, :]\n",
    "\n",
    "        w_new = int( ( aspect_ratio **.5 ) * scale * w )\n",
    "        h_new =  int( scale * w / ( aspect_ratio ** .5 ) )\n",
    "\n",
    "        anchor_boxes = torch.zeros_like(base_boxes)\n",
    "        \n",
    "        indices = product(\n",
    "            range(anchor_boxes.shape[0]), \n",
    "            range(anchor_boxes.shape[1])\n",
    "        )\n",
    "        \n",
    "        for i, j in indices: \n",
    "            \n",
    "            x, y, _, _ = base_boxes[i, j, :]\n",
    "            anchor_boxes[i, j, :] = torch.tensor([x, y, w_new, h_new])\n",
    "\n",
    "        return anchor_boxes\n",
    "    \n",
    "    anchor_boxes = []\n",
    "\n",
    "    for scale, aspect_ratio in product(scales, aspect_ratios):\n",
    "        \n",
    "        anchors = get_anchor_box_from_base(base_boxes_converted, scale, aspect_ratio)\n",
    "        \n",
    "        anchors = einops.repeat(\n",
    "            anchors, \n",
    "            'n_features_1 n_features_2 four -> n_features_1 n_features_2 1 four', \n",
    "            n_features_1 = feature_map_size, \n",
    "            n_features_2 = feature_map_size, \n",
    "            four = 4, \n",
    "        )\n",
    "        \n",
    "        anchor_boxes.append(anchors)\n",
    "        \n",
    "    anchor_boxes = torch.concat(anchor_boxes, dim = 2)    \n",
    "    \n",
    "    # CLIP TO THE BOUNDS OF THE INPUT IMAGE - \n",
    "    # THIS REQUIRES A FEW FORMAT CONVERSIONS\n",
    "    \n",
    "    n1, n2, k, four = anchor_boxes.shape\n",
    "    \n",
    "    anchor_boxes = einops.rearrange(\n",
    "        anchor_boxes, \n",
    "        'n1 n2 k four -> ( n1 n2 k ) four'\n",
    "    )\n",
    "\n",
    "    anchor_boxes = ops.box_convert( anchor_boxes, 'cxcywh', 'xyxy')\n",
    "\n",
    "    anchor_boxes = einops.rearrange(\n",
    "        anchor_boxes,\n",
    "        '( n1 n2 k ) four -> n1 n2 k four', \n",
    "        n1=n1, n2=n2, k=k, four=four\n",
    "    )\n",
    "\n",
    "    anchor_boxes = ops.clip_boxes_to_image(anchor_boxes, (input_size, input_size))\n",
    "\n",
    "    anchor_boxes = einops.rearrange(\n",
    "        anchor_boxes, \n",
    "        'n1 n2 k four -> ( n1 n2 k ) four'\n",
    "    )\n",
    "\n",
    "    anchor_boxes = ops.box_convert( anchor_boxes, 'xyxy', 'xywh')\n",
    "\n",
    "    anchor_boxes = einops.rearrange(\n",
    "        anchor_boxes,\n",
    "        '( n1 n2 k ) four -> n1 n2 k four', \n",
    "        n1=n1, n2=n2, k=k, four=four\n",
    "    )\n",
    "    \n",
    "    return anchor_boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The region proposal network consists of three main pieces:\n",
    "- a sliding window over the feature map, which also reduces its dimensionality\n",
    "- a collection of `k` anchor boxes corresponding to each position of this feature map\n",
    "- a classification head which attempts to classify each anchor box at each position as either background or object ('objectness score')\n",
    "- a box regression head which learns a parameterized transform to move the anchor boxes closer to the true boxes when they contain objects.\n",
    "\n",
    "Let's go over these components. Recall the network parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_input_size = 1024,\n",
    "feature_map_size = 32\n",
    "feature_dim = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have available the anchor boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([16., 16., 64., 64.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_boxes[1, 1, 0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with the sliding window, which is implemented as a simple 3 x 3 convolution. We have to choose the output feature dimension of this sliding window, which is normally smaller than the feature dimensions. In the paper, ReLUs are applied to the output of this convolution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliding_window = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels = feature_dim, \n",
    "        out_channels=hidden_dim, \n",
    "        kernel_size=3, \n",
    "        padding=1\n",
    "    ), \n",
    "    nn.ReLU()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try running it on some sample input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 32, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SANITY CHECK\n",
    "feature_map = torch.randn((batch_size, feature_dim, *(feature_map_size,) * 2))\n",
    "sliding_window(feature_map).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the bounding box regression component. A bounding box regressor learns parameters for a transform that maps a bounding box closer to the true bounding box. For each anchor box in each position, it will learn 4 transform parameters `t_x, t_y, t_w, t_h`. From these parameters, the anchor box `x_a, y_a, w_a, h_a` can be transformed to the proposed box `x, y, w, h` via:\n",
    "- `x = w_a * t_x + x_a`\n",
    "- `y = h_a * t_y + y_a `\n",
    "- `w = w_a * exp( t_w )`\n",
    "- `h = h_a * exp( t_h )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bounding box regressor is implemented using a 1x1 convolution layer followed by some simple rearranging to create the desired output, which is of the shape (b, 32, 32, k, 4). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = len(scales) * len(aspect_ratios)\n",
    "\n",
    "bbox_regressor = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=hidden_dim, \n",
    "        out_channels=k * 4, \n",
    "        kernel_size=1\n",
    "    ), \n",
    "    Rearrange(\n",
    "        'b (k four) h w -> b h w k four', \n",
    "        k = k, \n",
    "        four = 4\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure this produces the expected output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 32, 9, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "out_ = bbox_regressor( sliding_window(feature_map) ) \n",
    "out_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier is similar, but it only outputs two numbers per anchor, representing the scores for the class 0 (background) and 1 (object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Sequential(\n",
    "    nn.Conv2d(\n",
    "        in_channels=hidden_dim, \n",
    "        out_channels=k * 2, \n",
    "        kernel_size=1\n",
    "    ), \n",
    "    Rearrange(\n",
    "        'b (k two) h w -> b h w k two', \n",
    "        k = k, \n",
    "        two = 2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we call the model, we are interested in not just the regression scores, but the proposed boxes as well. The proposed boxes are computed by applying the transform parameterized by the scores to the corresponding anchor boxes. To apply the transform, the boxes tensor has to be flattened into dimensions (N, 4) - in our case n will be ( H W K )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxreg_transform(regression_scores, anchor_boxes, in_fmt='xywh'):\n",
    "    \"\"\"Apply the box regression transform along the last axis of the input.\n",
    "\n",
    "    Args:\n",
    "        regression_scores ([type]): a tensor of shape (N, 4), where the last dimension contains t_x, t_y, t_w, t_h\n",
    "        anchor_boxes ([type]): a tensor of shape (N, 4) specifying the anchor boxes upon which the transofrm is being performed.\n",
    "        in_fmt (str, optional): The format of the boxes. Defaults to 'xywh'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # the transform requires 'cxcywh' format:\n",
    "    anchor_boxes = ops.box_convert(anchor_boxes, in_fmt=in_fmt, out_fmt='cxcywh')\n",
    "    \n",
    "    x_a = anchor_boxes[:, 0]\n",
    "    y_a = anchor_boxes[:, 1]\n",
    "    w_a = anchor_boxes[:, 2]\n",
    "    h_a = anchor_boxes[:, 3]\n",
    "    \n",
    "    t_x = regression_scores[:, 0]\n",
    "    t_y = regression_scores[:, 1]\n",
    "    t_w = regression_scores[:, 2]\n",
    "    t_h = regression_scores[:, 3]\n",
    "    \n",
    "    x = ( t_x * w_a ) + x_a \n",
    "    y = ( t_y * h_a ) + y_a\n",
    "    w = w_a * torch.exp(t_w)\n",
    "    h = h_a * torch.exp(t_h)\n",
    "    \n",
    "    proposed_boxes = torch.stack( [x, y, w, h], dim=-1 )\n",
    "    proposed_boxes = ops.box_convert(anchor_boxes, in_fmt='cxcywh', out_fmt=in_fmt)\n",
    "    return proposed_boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to be able to reverse the transform by obtaining the transform scores given anchor boxes and the proposed boxes created by the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_boxreg_transform(proposed_boxes, anchor_boxes, in_fmt='xywh'):\n",
    "    \"\"\"Obtain the transform parameters that would transform the given anchor boxes to the given \n",
    "    output ( proposed boxes )\n",
    "\n",
    "    Args:\n",
    "        proposed_boxes (Tensor): A tensor of shape (N, 4)\n",
    "        anchor_boxes (Tensor): A tensor fo shape (N, 4)\n",
    "        in_fmt (str, optional): The format of the boxes. Defaults to 'xywh'.\n",
    "    \"\"\"\n",
    "    \n",
    "    anchor_boxes = ops.box_convert(anchor_boxes, in_fmt=in_fmt, out_fmt='cxcywh')\n",
    "    proposed_boxes = ops.box_convert(proposed_boxes, in_fmt=in_fmt, out_fmt='cxcywh')\n",
    "    \n",
    "    x_a = anchor_boxes[:, 0]\n",
    "    y_a = anchor_boxes[:, 1]\n",
    "    w_a = anchor_boxes[:, 2]\n",
    "    h_a = anchor_boxes[:, 3]\n",
    "    \n",
    "    x = proposed_boxes[:, 0]\n",
    "    y = proposed_boxes[:, 1]\n",
    "    w = proposed_boxes[:, 2]\n",
    "    h = proposed_boxes[:, 3]\n",
    "    \n",
    "    t_x = ( x - x_a ) / w_a\n",
    "    t_y = ( y - y_a ) / h_a \n",
    "    t_w = torch.log( w / w_a )\n",
    "    t_h = torch.log( h / h_a)\n",
    "    \n",
    "    transform_parameters = torch.stack( [t_x, t_y, t_w, t_h], dim=-1 )\n",
    "    return transform_parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the list of proposed boxes, the RPN applies the transform scores obtained by its bounding box regressor layer to its anchor boxes. To estimate the \"objectness\" of each of these box proposals, it uses the output of its classifier layer. When using the proposals and the objectness scores, we do not care from which feature map position they came, so we will actually collapse their dimensions from ( h, w, k, ... ) to ( h * w * k, ...). Also, because the loss function for training is obtained from the regression scores directly and not from the proposed boxes, we do not keep track of gradients during the regression transform. \n",
    "\n",
    "We can summarize this procedure in the following function: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def propose_boxes(in_feature_map, anchor_boxes):\n",
    "    \n",
    "    b, feature_dim, H, W = in_feature_map.shape\n",
    "    \n",
    "    sliding_window_output = sliding_window(in_feature_map)\n",
    "    \n",
    "    regression_scores = bbox_regressor(sliding_window_output)\n",
    "    objectness_scores = classifier(sliding_window_output)\n",
    "    \n",
    "    # expand anchor boxes into batch dimension\n",
    "    anchor_boxes = einops.repeat(\n",
    "        anchor_boxes,\n",
    "        ' h w k four -> b h w k four ', \n",
    "        b=b\n",
    "    )\n",
    "    \n",
    "    # have to fold the outer dimensions together to apply the transform:\n",
    "    b, h, w, k, four = regression_scores.shape\n",
    "    \n",
    "    regression_scores = einops.rearrange(\n",
    "        regression_scores, \n",
    "        'b h w k four -> ( b h w k ) four'\n",
    "    )\n",
    "    \n",
    "    anchor_boxes = einops.rearrange(\n",
    "        anchor_boxes,  \n",
    "        'b h w k four -> ( b h w k ) four'\n",
    "    )\n",
    "    \n",
    "    objectness_scores = einops.rearrange(\n",
    "        objectness_scores,  \n",
    "        'b h w k two -> ( b h w k ) two'\n",
    "    )\n",
    "    \n",
    "    proposed_boxes = boxreg_transform(regression_scores, anchor_boxes)\n",
    "    \n",
    "    proposed_boxes = einops.rearrange(\n",
    "        proposed_boxes, \n",
    "        '( b h w k ) four -> b ( h w k ) four', \n",
    "        b=b, h=h, w=w, k=k, four=4\n",
    "    )\n",
    "    \n",
    "    regression_scores = einops.rearrange(\n",
    "        regression_scores, \n",
    "        '( b h w k ) four -> b ( h w k ) four', \n",
    "        b=b, h=h, w=w, k=k, four=4\n",
    "    )\n",
    "    \n",
    "    objectness_scores = einops.rearrange(\n",
    "        objectness_scores, \n",
    "        ' ( b h w k ) two -> b ( h w k ) two', \n",
    "        b=b, h=h, w=w, k=k, two=2\n",
    "    )\n",
    "    \n",
    "    anchor_boxes = einops.rearrange(\n",
    "        anchor_boxes, \n",
    "        ' ( b h w k ) four -> b ( h w k ) four', \n",
    "        b=b, h=h, w=w, k=k, four=4\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'proposed_boxes': proposed_boxes, \n",
    "        'regression_scores': regression_scores,\n",
    "        'objectness_scores': objectness_scores, \n",
    "        'anchor_boxes': anchor_boxes\n",
    "    } \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.0000,   0.0000,  48.0000,  48.0000],\n",
       "         [  0.0000,   0.0000,  61.0000,  38.5000],\n",
       "         [  0.0000,   0.0000,  38.5000,  61.0000],\n",
       "         ...,\n",
       "         [880.0000, 880.0000, 144.0000, 144.0000],\n",
       "         [827.0000, 917.5000, 197.0000, 106.5000],\n",
       "         [917.5000, 827.0000, 106.5000, 197.0000]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "propose_boxes( feature_map, anchor_boxes )['proposed_boxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the base output of our region proposal network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the RPN, the goal is to improve the objectness prediction and the regression scores generated by the network. To do so, we need to apply a loss function between these scores and some targets. We have objectness scores and regression scores for every single anchor box - however, we only have a small number of ground truth bounding boxes with each image. The way we generate positive and negative training examples is to look at the filtered boxes and mark them as positive (matching a ground truth box), negative (background) or unknown. This is done according to the IoU Filtering algorithm. We went through the implementation of this algorith in the \"region proposal\" notebook, and we copy it below for convenience: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_proposed_boxes_to_true(\n",
    "        true_boxes: torch.Tensor, \n",
    "        proposed_boxes: torch.Tensor, \n",
    "        min_num_positives: int, \n",
    "        in_format: str = 'xywh', \n",
    "        true_box_labels: torch.Tensor = None, \n",
    "        positivity_threshold: float = 0.7, \n",
    "        negativity_threshold: float = 0.3\n",
    "    ):\n",
    "    \"\"\"Matches proposed bounding boxes to a tensor of ground truth bounding boxes\n",
    "       and returns a tensor of labels indicating positive (1, object) or negative \n",
    "       (0, no object) or inconclusive (-1) for each match based on whether \n",
    "       a certain IoU threshold with a ground truth box is met. This labeling is done\n",
    "       according to specified thresholds and also with a specified minimum number \n",
    "       of positives. If the positivity threshold does not generate enough positives,\n",
    "       they will be generated by choosing the ones with the best overlap.\n",
    "\n",
    "    Args:\n",
    "        true_boxes (torch.Tensor): A tensor of boxes of shape (N, 4)\n",
    "        \n",
    "        proposed_boxes (torch.Tensor): A tensor of boxes of shape (M, 4)\n",
    "        \n",
    "        min_num_positives (int): minimum number of positives generated by the matching\n",
    "        \n",
    "        in_format (str, optional): string specifying the string format - \n",
    "        see torchvision ops documentation.Defaults to 'xywh'.\n",
    "        \n",
    "        box_labels (torch.Tensor, optiona): tensor of shape (N) giving the class labels\n",
    "        corresponding with the ground truth boxes. \n",
    "        \n",
    "        positivity_threshold (float, optional): Above this threshold a proposed box will \n",
    "        be considered to match with the ground truth. Defaults to 0.7.\n",
    "        \n",
    "        negativity_threshold (float, optional): below this threshold a box will be considered \n",
    "        to be background. Defaults to 0.3.\n",
    "\n",
    "    Returns:\n",
    "        [type]: [description]\n",
    "    \"\"\"\n",
    "    assert len(true_boxes.shape) == 2\n",
    "    assert len(proposed_boxes.shape) == 2\n",
    "    \n",
    "    num_true_boxes, _ = true_boxes.shape\n",
    "    num_proposed_boxes, _ = proposed_boxes.shape\n",
    "\n",
    "    ious = ops.box_iou(\n",
    "        ops.box_convert(proposed_boxes, in_fmt=in_format, out_fmt='xyxy'),\n",
    "        ops.box_convert(true_boxes, in_fmt=in_format, out_fmt='xyxy')\n",
    "    )\n",
    "    \n",
    "    max_ious = torch.max(ious, dim=-1, )\n",
    "    matching_true_boxes = true_boxes[max_ious.indices]\n",
    "    if true_box_labels is not None:\n",
    "        matching_true_box_labels = true_box_labels[max_ious.indices]\n",
    "    \n",
    "    labels = (torch.ones_like(max_ious.values) * -1).long()\n",
    "    indices = torch.tensor(range(len(labels)))\n",
    "\n",
    "    positive_indices = indices[max_ious.values >= positivity_threshold]\n",
    "    if len(positive_indices) < min_num_positives:\n",
    "        positive_indices = torch.sort(max_ious.values, dim=-1, descending=True).indices[:min_num_positives]\n",
    "\n",
    "    if true_box_labels is not None: \n",
    "        labels[positive_indices] = matching_true_box_labels[positive_indices] \n",
    "    else:\n",
    "        labels[positive_indices] = 1\n",
    "\n",
    "    negative_indices = indices[max_ious.values < negativity_threshold]\n",
    "\n",
    "    labels[negative_indices] = 0\n",
    "    \n",
    "    return {\n",
    "        'matching_true_boxes': matching_true_boxes, \n",
    "        'proposed_boxes': proposed_boxes,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the RPN, we would get a batch which consists of ground truth boxes and input feature maps. Instead of storing the ground truth bounding boxes as a tensor with the batch size in the 0'th dimension, we need to store them either as a list of tensors, as a tensor with an extra column indexing which batch it came from, or as a tensor together with an indexing tensor - otherwise we may be dealing with the issue of tensors with different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_boxes(num_boxes, format='xywh'):\n",
    "\n",
    "    xy = torch.randint(0, 100, (num_boxes, 2))\n",
    "    wh = torch.randint_like(xy, 200) + xy\n",
    "    \n",
    "    boxes = torch.concat([xy, wh], dim=-1)\n",
    "    return ops.box_convert(boxes, in_fmt='xyxy', out_fmt=format)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the kind of thing we need to do to batch the ground truth boxes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "\n",
    "gt_boxes_b1 = random_boxes(2)\n",
    "gt_boxes_b2 = random_boxes(3)\n",
    "\n",
    "true_boxes = [gt_boxes_b1, gt_boxes_b2]\n",
    "\n",
    "indices = []\n",
    "for i in range(len(true_boxes)):\n",
    "    indices.extend( [i] * len(true_boxes[i]) )\n",
    "indices = torch.tensor( indices )\n",
    "true_boxes = torch.concat(true_boxes, axis=0)\n",
    "\n",
    "in_feature_map = torch.randn( (2, 1024, 32, 32) )\n",
    "\n",
    "batch = feature_map, true_boxes, indices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above will have to be implemented as part of a manual collate-fn in the dataloader for this network. For now, let's focus on taking computing the loss. This happens in 3 steps: \n",
    "- The network computes the proposed boxes, objectness and regression scores.\n",
    "- The proposed boxes are filtered with reference to the ground truth boxes to label them as negative, positive, or unknown\n",
    "- The loss is computed based on localization and classification losses between the proposed boxes and matched true boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 9216, 4]),\n",
       " torch.Size([2, 9216, 4]),\n",
       " torch.Size([2, 9216, 2]),\n",
       " torch.Size([2, 9216, 4]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict = propose_boxes(in_feature_map, anchor_boxes)\n",
    "out_dict['proposed_boxes'].shape, out_dict['regression_scores'].shape, out_dict['objectness_scores'].shape, out_dict['anchor_boxes'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our IoU matching algorithm works on a single batch at a time. Let's go through it for the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x105812d70>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 88,  26, 153,  18],\n",
       "        [ 38,  97,   7,  21]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_boxes_for_batch = true_boxes[indices == batch]\n",
    "true_boxes_for_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_box_output = match_proposed_boxes_to_true(\n",
    "    true_boxes_for_batch, \n",
    "    out_dict['proposed_boxes'][batch], \n",
    "    min_num_positives=64\n",
    ")\n",
    "\n",
    "labels_for_batch = match_box_output['labels']\n",
    "matching_true_boxes_for_batch = match_box_output['matching_true_boxes']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also have to compute the regression scores that correspond to the matching ground truth boxes relative to the anchor boxes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_regression_scores = inverse_boxreg_transform(match_box_output['matching_true_boxes'], out_dict['anchor_boxes'][batch])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to select the subset of these scores upon which the loss will be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_for_loss = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_for_loss = torch.tensor(range(len(labels_for_batch))).long()\n",
    "\n",
    "indices_for_loss_positive = indices_for_loss[labels_for_batch == 1]\n",
    "indices_for_loss_negative = indices_for_loss[labels_for_batch == 0]\n",
    "\n",
    "indices_for_loss_positive = indices_for_loss_positive[torch.randperm(len(indices_for_loss_positive))]\n",
    "if len(indices_for_loss_positive) >= num_examples_for_loss//2:\n",
    "    indices_for_loss_positive = indices_for_loss_positive[:num_examples_for_loss//2]\n",
    "\n",
    "indices_for_loss_negative = indices_for_loss_negative[torch.randperm(len(indices_for_loss_negative))]\n",
    "indices_for_loss_negative = indices_for_loss_negative[:num_examples_for_loss - len(indices_for_loss_positive)]\n",
    "\n",
    "indices_for_loss = torch.concat([indices_for_loss_negative, indices_for_loss_positive])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9216, 4])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_dict['regression_scores'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scores_for_loss = out_dict['regression_scores'][batch][indices_for_loss]\n",
    "target_reg_scores_for_loss = target_regression_scores[indices_for_loss]\n",
    "\n",
    "objectness_scores_for_loss = out_dict['objectness_scores'][batch][indices_for_loss]\n",
    "labels_for_loss = labels_for_batch[indices_for_loss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall algorithm for selecting training examples is given below: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_training_examples(\n",
    "    proposed_boxes: torch.Tensor, \n",
    "    anchor_boxes: torch.Tensor, \n",
    "    regression_scores: torch.Tensor, \n",
    "    objectness_scores: torch.Tensor, \n",
    "    true_boxes: torch.Tensor, \n",
    "    num_training_examples,\n",
    "    min_num_positives, \n",
    "    positivity_threshold: float = 0.7, \n",
    "    negativity_threshold: float = 0.3, \n",
    "    true_box_labels = None\n",
    "):\n",
    "\n",
    "    box_matching_output = match_proposed_boxes_to_true(\n",
    "        true_boxes, \n",
    "        proposed_boxes, \n",
    "        min_num_positives, \n",
    "        true_box_labels=true_box_labels,\n",
    "        positivity_threshold=positivity_threshold, \n",
    "        negativity_threshold=negativity_threshold\n",
    "    )\n",
    "\n",
    "    labels = box_matching_output['labels']\n",
    "    matching_true_boxes = box_matching_output['matching_true_boxes']\n",
    "    \n",
    "    target_regression_scores = inverse_boxreg_transform(\n",
    "        matching_true_boxes, anchor_boxes\n",
    "    )\n",
    "    \n",
    "    indices_for_loss = torch.tensor(range(len(labels))).long()\n",
    "\n",
    "    indices_for_loss_positive = indices_for_loss[labels > 0]\n",
    "    indices_for_loss_negative = indices_for_loss[labels == 0]\n",
    "\n",
    "    indices_for_loss_positive = indices_for_loss_positive[torch.randperm(len(indices_for_loss_positive))]\n",
    "    if len(indices_for_loss_positive) >= num_training_examples//2:\n",
    "        indices_for_loss_positive = indices_for_loss_positive[:num_training_examples//2]\n",
    "\n",
    "    indices_for_loss_negative = indices_for_loss_negative[torch.randperm(len(indices_for_loss_negative))]\n",
    "    indices_for_loss_negative = indices_for_loss_negative[:num_training_examples - len(indices_for_loss_positive)]\n",
    "\n",
    "    indices_for_loss = torch.concat([indices_for_loss_negative, indices_for_loss_positive])\n",
    "    \n",
    "    reg_scores_for_loss = regression_scores[indices_for_loss]\n",
    "    target_reg_scores_for_loss = target_regression_scores[indices_for_loss]\n",
    "\n",
    "    objectness_scores_for_loss = objectness_scores[indices_for_loss]\n",
    "    labels_for_loss = labels[indices_for_loss]\n",
    "\n",
    "    return {\n",
    "        'regression_scores': reg_scores_for_loss, \n",
    "        'target_regression_scores': target_reg_scores_for_loss, \n",
    "        'objectness_scores': objectness_scores_for_loss, \n",
    "        'labels': labels_for_loss,\n",
    "        'indices': indices_for_loss,\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "training_examples = select_training_examples(\n",
    "    out_dict['proposed_boxes'][batch], \n",
    "    out_dict['anchor_boxes'][batch],\n",
    "    out_dict['regression_scores'][batch],\n",
    "    out_dict['objectness_scores'][batch],\n",
    "    true_boxes[indices == batch],\n",
    "    num_training_examples=256,\n",
    "    min_num_positives=64, \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the loss function for the batch using the loss function, which is a combination of cross-entropy loss between the objectness scores and targets,\n",
    "as well as the smooth l1 score between the regression and target regression scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is fairly straightforward, it is the sum of the localization and classification loss. The classification loss is the cross-entropy between the objectness scores and the target labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7276, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def classification_loss(class_scores, labels):\n",
    "    return F.cross_entropy(class_scores, labels.long())\n",
    "\n",
    "classification_loss(objectness_scores_for_loss, labels_for_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression loss is obtained using the \"smooth L1\" norm of the difference between the true regression scores and the target regression scores, but is only applied to those boxes which are not in the background class. Here is the smooth l1 norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_l1_norm(tensor, dim=-1):\n",
    "    \"\"\"Calculates the smooth l1 norm along the specified dimension of the tensor\"\"\"\n",
    "    x = tensor\n",
    "    x = torch.where(\n",
    "        torch.abs(x) < 1, \n",
    "        .5 * x**2,\n",
    "        torch.abs(x) - 0.5\n",
    "    )\n",
    "    \n",
    "    return torch.sum(x, dim=dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the regression loss (which requires the labels together with the regression scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_regression_loss(regression_scores, target_regression_scores, labels):\n",
    "    \n",
    "    # select non-background indices \n",
    "    non_background_indices = labels != 0\n",
    "    \n",
    "    regression_scores = regression_scores[non_background_indices]\n",
    "    target_regression_scores = target_regression_scores[non_background_indices]\n",
    "    \n",
    "    norms = smooth_l1_norm(regression_scores - target_regression_scores)\n",
    "    \n",
    "    # get average l1 norm \n",
    "    return einops.reduce( norms, 'n -> ()', 'mean')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8550], grad_fn=<ReshapeAliasBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_regression_loss(reg_scores_for_loss, target_reg_scores_for_loss, labels_for_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They overall loss is their sum, weighted by a parameter lambda which is specified as a hyperparameter of the training system. Let us write it out as a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RCNNLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, lambda_=1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, class_scores, regression_scores, \n",
    "                target_regression_scores, labels):\n",
    "        \n",
    "        cls_loss = classification_loss(class_scores, labels)\n",
    "        loc_loss = box_regression_loss(regression_scores, target_regression_scores, labels)    \n",
    "        \n",
    "        return cls_loss + self.lambda_ * loc_loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5826], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RCNNLoss()(objectness_scores_for_loss, reg_scores_for_loss,\n",
    "           target_reg_scores_for_loss, labels_for_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.5826], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RCNNLoss()(training_examples['objectness_scores'], training_examples['regression_scores'], \n",
    "           training_examples['target_regression_scores'], training_examples['labels']\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the RPN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume we have a fully-trained RPN that predicts accurate proposed boxes together with objectness scores. To use this box proposals, we must filter them in two ways:\n",
    "- throwing away box proposals that overlap too much with other box proposals with greater objectness scores (non-max suppression)\n",
    "- Throwing away boxes that do not have a high enough objectness score to be considered an object. \n",
    "\n",
    "Let's go through both of these steps on the box proposals of a single image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the out-dict containing proposed boxes with objectness scores. Let's use the first image from the batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_boxes = out_dict['proposed_boxes'][0]\n",
    "objectness_scores = out_dict['objectness_scores'][0]\n",
    "\n",
    "iou_threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([9216, 4]), torch.Size([9216, 2]))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proposed_boxes.shape, objectness_scores.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert the objectness scores to foreground probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_prob = F.softmax(objectness_scores, dim=-1)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply nms based on the object probabilities. Torchvision expects the box format to be 'xyxy', so we have to convert first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_keep = ops.nms(ops.box_convert(proposed_boxes, in_fmt='xywh', out_fmt='xyxy'), object_prob, iou_threshold=iou_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposed_boxes = proposed_boxes[indices_to_keep]\n",
    "objectness_scores = objectness_scores[indices_to_keep]\n",
    "object_prob = object_prob[indices_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nms_to_region_proposals(\n",
    "    proposed_boxes: torch.Tensor, \n",
    "    objectness_scores: torch.Tensor,\n",
    "    iou_threshold: float,\n",
    "):\n",
    "    \"\"\"applies nms to remove overlapping boxes with a lower objectness score\n",
    "\n",
    "    Args:\n",
    "        proposes_boxes (torch.Tensor): (N, 4) tensor of boxes in format xywh\n",
    "        objectness_scores (torch.Tensor): (N, 2) tensor of objectness scores\n",
    "        iou_threshold: The iou threshold for proposed boxes to be considered overlapping. \n",
    "        \n",
    "    Returns: \n",
    "        a dict containing the proposed boxes, the objectness scores, and the object probability\n",
    "        after nms. \n",
    "    \"\"\"\n",
    "    \n",
    "    object_prob = F.softmax(objectness_scores, dim=-1)[:, 1]\n",
    "    \n",
    "    indices_to_keep = ops.nms(\n",
    "        ops.box_convert(proposed_boxes, in_fmt='xywh', out_fmt='xyxy'),\n",
    "        object_prob,\n",
    "        iou_threshold=iou_threshold\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'proposed_boxes': proposed_boxes[indices_to_keep],\n",
    "        'objectness_scores': objectness_scores[indices_to_keep],\n",
    "        'object_probs': object_prob[indices_to_keep]\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_output = apply_nms_to_region_proposals(\n",
    "    out_dict['proposed_boxes'][0],\n",
    "    out_dict['objectness_scores'][0], \n",
    "    iou_threshold=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'proposed_boxes': tensor([[707.0000, 121.5000,  90.0000,  45.0000],\n",
       "         [  0.0000, 496.0000, 176.0000, 256.0000],\n",
       "         [661.5000, 643.0000, 181.0000,  90.0000],\n",
       "         ...,\n",
       "         [688.0000, 400.0000,  64.0000,  64.0000],\n",
       "         [176.0000, 176.0000,  64.0000,  64.0000],\n",
       "         [ 99.0000,  85.5000,  90.0000, 181.0000]]),\n",
       " 'objectness_scores': tensor([[-0.7585,  0.5623],\n",
       "         [-0.8518,  0.4643],\n",
       "         [-0.6227,  0.4560],\n",
       "         ...,\n",
       "         [ 0.4460, -0.5735],\n",
       "         [ 0.5149, -0.5081],\n",
       "         [ 0.6155, -0.6413]], grad_fn=<IndexBackward0>),\n",
       " 'object_probs': tensor([0.7893, 0.7885, 0.7462,  ..., 0.2651, 0.2644, 0.2215],\n",
       "        grad_fn=<IndexBackward0>)}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nms_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.map import MeanAveragePrecision\n",
    "\n",
    "metric = MeanAveragePrecision(box_format='xywh')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_preds = [{\n",
    "    'boxes': nms_output['proposed_boxes'], \n",
    "    'scores': nms_output['object_probs'], \n",
    "    'labels': torch.IntTensor([0] * len(nms_output['proposed_boxes']))\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_targets = [\n",
    "    {'boxes': true_boxes, 'labels': torch.IntTensor([0]* len(true_boxes))}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric(metric_preds, metric_targets)['map']"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "076c5b3100cf5c21b2117d6c29f9acded3ff1582821caca1987db7343f90d29e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lungbot': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
